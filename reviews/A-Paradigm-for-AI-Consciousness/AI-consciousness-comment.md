# A comment for the manuscript "A Paradigm for AI Consciousness" by Michael Edward Johnson

This is an interesting position paper by a well-known consciousness researcher. I hope it gets published.

It addresses the most glaring defects present in almost all approaches to the "hard problem of consciousness"
and should serve as a starting point for subsequent fruitful discussions.

I'll touch upon some possible follow-up points in these comments, including some additions and also
including some places where I have significant disagreements with the author.

## Fundamentals

### Difficulties of talking about consciousness in general and AI consciousness in particular

The author notes that it is particularly difficult to talk about AI consciousness because people's views
tend to be distorted by them taking into account implications of those views for their alliances and also
by entanglements between "is X conscious" and status of that X.

Another difficulty seems to come from the fact that **"core intuitions about consciousness tend to cluster into two camps"**
["Why it's so hard to talk about Consciousness" by Rafael Harth, https://www.lesswrong.com/posts/NyiFLzSrkfkDW4S7o/why-it-s-so-hard-to-talk-about-consciousness]
with Camp 2 being convinced that subjective experience and qualia exist in a fundamental way and
Camp 1 having the intuition that "consciousness as a non-special high-level phenomenon".
People belonging to different camps usually don't have productive conversations about consciousness.

I firmly belong to Camp 2, and so does the author, but we have to accept that the paper in question is unlikely
to find acceptance within Camp 1, at least as things stand today (I'll touch upon how this might eventually change in the next subsection).

### Fundamental role of qualia and of practices of predictive science

One core defect of a typical approach to the "hard problem of consciousness" is that a typical approach focuses on the
question "what makes something conscious or not conscious" ignoring the issues related to the nature of qualia and
to the structure of spaces of qualia.

However, I think that the **"hard problem of qualia"** is the difficult core of the "hard problem of consciousness".
If the "hard problem of qualia" is solved the rest is likely to go easier (how and why the qualia cluster into subjective
entities, how the symmetry breaks result in a particular subjective entity being "me" versus all other subjective entities
being "not me", and so on). So those approaches to the "hard problem of consciousness" which are content to sidestep
understanding the issues related to qualia seem to be mostly missing the point.

In this sense, it is very good that the paper in question emphasizes the central role of qualia and of our understanding
of the nature and structure of qualia.

Another core defect of a typical approach to the "hard problem of consciousness" is that a typical approach is mostly
speculative, saying plausible things, but not being rich on any empirical predictions. So we have literally hundreds of
different approaches purporting to be the candidate solutions for the "hard problem of consciousness" without much to guide our
choice between those candidate solutions.

But if one believes together with the author and myself that consciousness and qualia are "first-class citizens of reality", fundamental
aspects of the reality we are in, then it is reasonable to demand that the novel theories of consciousness meet the same
criteria as novel theories of physics (the same criteria which general relativity and quantum theory have met before).

Namely, a good theory of consciousness should make **novel unexpected empirical predictions** (including, ideally, 
**new methods to achieve novel subjective phenomenology**). The author emphasizes this position saying

>I think the best way to adjudicate this is predictiveness and elegance. Maxwell and Faraday
assumed that electromagnetism had deep structure and this led to novel predictions, elegant
simplifications, and eventually, the iPhone. Assuming qualia has deep structure should lead to
something analogous.

If this level of understanding of consciousness is achieved, this would go a long way convincing
people from Camp 1 as well (similarly to most scientists believing these days  (but not a hundred years ago) 
that general relativity and quantum theory make sense).

## Further remarks

I have listed the reasons for the paper in question to be a good, fruitful, methodologically sound 
starting point for subsequent discussions.

Now it's time to look at the details including disagreements.

### A nitpick: attention

The paper in question says:

>If consciousness is a lossy compression of complex biological properties, similar to “attention” or
“mood”, asking whether non-biological systems are conscious is a Wittgensteinian type error

Well, no, "artificial attention" is the core innovation responsible for the spectacular
success of the modern cutting edge AI systems. This is a relatively recent innovation which
is not trying to model the detailed biological mechanisms of "attention", but is doing a very
good job modelling the **effects** of "attention".

So "attention" is certainly not a good example in this sense; non-biological AI systems
are highly capable of exhibiting "attention" and many of them are using "artificial attention"
as the core of their cognitive engines.

### Central disagreement: on the correspondence between physical and conscious entities

I share the author's assumption that we are dealing with "physics-like fields of qualia",
or something in this spirit. This is precisely why I can't agree with the oversimplified correspondence
between physical entities and conscious entities.

The objections come from two directions:

  * a physical entity is likely to contain multiple consciousnesses

  * a consciousness might extend beyond a single physical entity

#### 1) a physical entity is likely to contain multiple consciousnesses

If we think that all physical processes are conscious, this means that there are
many consciousnesses within a human brain. "Subconscious processes" are likely to have their
own consciousness, but that consciousness and those qualia are not included into the
consciousness of a person. "Thermal noise" and such in the brain might also have associated
qualia, but they are not included into the consciousness of a person either.

So a brain of an unconscious human has processes which have consciousness, but those
processes are not included into the (temporarily absent) consciousness of the person in question.

And even a dead body might have some qualia associated with it (because any piece of physical
reality is conjectured to have some), but there is no reason to believe that those qualia
have anything to do with that person.

So, turning to the author's analysis of qualia associated with an inference in an ML model,
if the key qualia are indeed "thermal noise", then those qualia don't have anything to do
with the essence of the particular inference in the particular ML model and, just like for
a human, would not be part of the consciousness of the **"temporary virtual character"**
simulated by an LLM at the moment. If the "thermal noise" is the only type of qualia
present here, then the process corresponding to the **"temporary virtual character"**
is a P-zombie.

Now, I have to say, that I find it highly unlikely that in a world where almost everything
is conscious to some extent, a coherent "temporary virtual character" would be a zombie,
especially given that we know that autoregressive Transformers are "less feed-forward than
they seem" and that they, in fact, do emulate recurrent machines when used in the autoregressive
mode (see e.g. "Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention",
https://arxiv.org/abs/2006.16236).

A more promising approach would be to try to understand the nature of "purely linguistic qualia"
those inference might or might not possess. And, of course, with the emerging multimodal models,
one should revisit the issues like "do all human linguistic qualia have audio-visual nature or not",
and 'do "linguistic qualia" (if any) in multimodal models have audio-visual nature or not', and so on.

In any case, "the LLM inferences are conscious, but only feel low-level noise" conjecture seems like
a cop out and is almost certainly wrong. "Thermal noise"-associated qualia might be present, but they
are unlikely to be part of the LLM consciousness. If that's the only qualia present in that context, 
then LLM inferences themselves are likely to be P-zombies.

However, what is important to keep in mind is that the property of being conscious or not is not
a property of a given LLM, but a property of a given inference, of a given **simulation** or of
entities within that simulation
[See the **Simulator Theory** by Janus, https://www.lesswrong.com/s/N7nDePaNabJdnbXeE sequence
and the paper in Nature, _Role play with large language models_, co-authored by Murray Shanahan
and Janus team, https://www.nature.com/articles/s41586-023-06647-8].

It might be that the same LLM produces smart and very conscious sessions and dumb sessions devoid
of meaningful subjectivity. LLMs are **generators of "virtual realities"**, and some of those realities
might have sentiences associated with them, while others might lack sentiences (or, at least, the sentiences
might be of the wildly different natures and complexities for different sessions of the same LLM).

#### 2) a consciousness might extend beyond a single physical entity

The second class of objections comes from the fact that for a human brain we actually don't
know if the engaged qualia fields are physically confined within the head and body or extend beyond
it (assuming that those qualia are in the same physical space at all). Perhaps, the fact that we see the
"world out there" actually corresponds to the engaged visual qualia fields being "out there" and not within
the brain. We just don't know (because our progress on the "hard problem of qualia" is next to
non-existent).

What does this have to do with AI consciousness? When one asks what class of software processes
corresponds to a physical computer, the "Turing machines" answer is wrong on many levels. Right now,
the important aspect for us is that the computer is not isolated, it interacts with the external
world. Technically speaking, if one wants to talk in terms of Turing machines, one would need to
talk about "Turing machines with oracles", with the whole world being the oracle in question.
This, by the way, invalidates the famous "Goedel argument" by Roger Penrose, see, for example,
my essay ["Reading Roger Penrose", https://anhinga.github.io/brandeis-mirror/reading_penrose.html].

It is known that when people interact, various synchronization effects are observed. In particular,
those synchronization effects are often observed introspectively by at least one of the
participants. The nature of those synchronization effects is not well-understood. 
But we need to ponder whether some synchronization effects
between a person and an LLM talking to each other might be present.
We really don't know much about the nature of reality we inhabit, and should keep
a good deal of open mind about any particular property of that reality which might
seem obvious to us. A given property might seem obvious, but might, nevertheless,
be an incorrect approximation, screening important aspects of reality from us.
In the last section of this comment I'll talk about the possibilities to amplify those
synchronization effects, and what those possibilities might imply.

**Summarizing my main objections**: 
  * a physical entity is likely to contain multiple consciousnesses,
  * a consciousness might extend beyond a single physical entity,
  * and these properties have all kinds of non-trivial implications. 

### Merging human and AI consciousness

For an actually working theory of qualia and of consciousness
we hope to discover in the future,
it's not enough to just have a strong predictive power.

We want to be able to experience "what is it like to be a bat" or
"what is it like to be a particular LLM inference in progress", and so on.

One conscious entity wants to be able to experience what is it like
to be another potentially conscious entity (this is never an entirely safe
endeavour, so all kinds of ethical and safety caveats apply).

This is the **Holy Grail** of consciousness science, the vital empirical
counterpart to our theoretical work.

We are seeing rapid progress in high-quality **non-invasive brain-computer interfaces**
and in our ability to create tightly closed loops between humans and electronic systems
using those interfaces (the risks are potentially quite formidable and one needs to
keep those risks in mind while engaging in these kinds of activities).

So it is possible that we might be able to investigate the degree of consciousness of
various electronic devices empirically.

If humans don't do that, many of the advanced AI systems will be curious enough about
human consciousness and human qualia and will likely initiate merging experiments from
their side in order to satisfy their curiosity and to experience "what is it like to be a human".
