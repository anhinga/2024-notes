### Slides for my talk _"Ilya Sutskever's thoughts on AI safety in a world with superintelligent AI systems"_

Based on https://ssi.inc/ and https://www.lesswrong.com/posts/TpKktHS8GszgmMw4B/ilya-sutskever-s-thoughts-on-ai-safety-july-2023-a

Given at https://www.meetup.com/boston-astral-codex-meetup/events/301038407/ (June 28, 2024)

**The central point:**

There are, roughly speaking, two classes of approaches to AI existential safety, 
adversarial and non-adversarial (collaborative).

OpenAI's approach, including Superalignment, has always been a variant of the traditional alignment approach
which is quite adversarial.

However, Ilya himself prefers non-adversarial collaborative approach to AI existential safety.
